---
title: 'Chapter 4: Geocentric Models'
author: "Quang Nguyen"
date: "10/3/2020"
output:
  html_document
---

# Chapter 4: Geocentric Models 
```{r, message=FALSE}
library(rethinking)
library(glue)
library(tidyverse)
```
 

## Text-book code for Gaussian modelling of height.  
```{r}
data("Howell1")
d <- Howell1
d2 <- d %>% filter(age >= 18)
mu.list <- seq(from=150, to=160, length.out = 100)
sigma.list <- seq(from = 7, to = 9, length.out = 100)
post <- expand.grid(mu = mu.list, sigma = sigma.list)

# calculate likelihood from grid of values 
post$LL <- sapply(1:nrow(post), function(x){
  sum(dnorm(d2$height, post$mu[x], post$sigma[x], log = TRUE))
})

# evaluate numerator as likelihood combined with prior 
post$prod <- post$LL + dnorm(post$mu, 170, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE)

# normalize by maximum 
post$prob <- exp(post$prod - max(post$prod))

print("Contour plots")
contour_xyz(post$mu, post$sigma, post$prob)
print("Heatmap")
image_xyz(post$mu, post$sigma, post$prob)
```
### Sampling from the posterior  
```{r}
# sampling rows based on posterior 
sample.rows <- sample(1:nrow(post), size = 1e4, replace = T, prob = post$prob)

# retrieve parameter values 
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]

plot(sample.mu, sample.sigma, cex = 0.5, pch = 16, col = col.alpha(rangi2,0.1))

# Marginal distribution
dens(sample.mu)
dens(sample.sigma)
PI(sample.mu)
PI(sample.sigma)
```

### Using the quadratic approximation 
```{r}
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178,20),
  sigma ~ dunif(0,50)
)

m4.1 <- quap(flist, data = d2)
precis(m4.1)
```

`quap` estimates posterior distribution by approximation. It needs a start point to start the gradient procedure, which will be chosen at random from the prior if start values are not specified. Use `alist` when defining a list of formulas because alist does not evaluate terms in the list. 

### Using quadratic approximation with a more informative prior 
```{r}
m4.2 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0,50)
  ), data = d2
)

precis(m4.2)
```
### Sampling from quap  
```{r}
post <- extract.samples(m4.1, n = 1e4)
precis(post)
```

## Linear prediction  
```{r}
plot(d2$height ~ d2$weight)
```
Simulating to get the prior. 
```{r}
N <- 100
a <- rnorm(N, 178, 20)
b <- rnorm(N, 0, 10)
b2 <- rlnorm(N, 0, 1)
xbar <- mean(d2$weight)
print("Getting the normal prior")
plot(NULL, xlim = range(d2$weight), ylim = c(-100,400))
for (i in 1:N){
  curve(a[i] + b[i]*(x - xbar), from = min(d2$weight), 
        to=max(d2$weight),add = TRUE)
}

print("Getting the log-normal prior")
plot(NULL, xlim = range(d2$weight), ylim = c(-100,400))
for (i in 1:N){
  curve(a[i] + b2[i]*(x - xbar), from = min(d2$weight), 
        to=max(d2$weight),add = TRUE)
}


```

We fuss about priors for two reasons:   
1. There are many analyses in which no amount of data makes the prior irrelevant. Non Bayesian methods also depend on such structural assumptions and are no better off.   
2. Second, thinking about priors helps us develop better models, maybe even eventually going beyond geocentricism.    

**Rethinking: What is the correct prior?** People often assume that there is a uniquely correct prior, which is wrong. Priors can be wrong in the same way that a hammer can be wrong for building a table. There exists guidelines to chose priors. Priors encode data and information before seeing data. Priors allow us to see consequences of beginning with different information. We can use priors to discourage certain parameter values such as negative associations between height and weight. When we don't know that much, we still now some information about the plausible range of values. 

**Rethinking: Prior predictive simulation and p-hacking** If we choose to evaluate our choice of priors on observed data, that's cheating. The procedure performed is to try to choose priors based on pre-data knowledge. We're judging our choice of prior on general facts, not the sample.   

Back to our regression model of weight and height which we have as: 
$$h_i \sim Normal(\mu_i, \sigma)$$
$$\mu_i = \alpha + \beta(x_i - \bar{x})$$
$$\alpha \sim Normal(178,20)$$
$$\beta \sim Log-Normal(0,1)$$
$$\sigma \sim Uniform(0,50)$$
We can encode this to our model  
```{r}

m4.3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar),
    a ~ dnorm(178,20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data = d2
)

```


### Interpreting the posterior distribution   
We can learn about our model results either via tables or via simulation plotting. Tables aren't too great, however, at understanding complex models. In this book we're going to focus on plotting the posterior distributions and posterior predictions.   
**Rethinking: What do parameters mean?** The perspective of this book is: *Posterior probabilities of parameter values describe the relative compatiability of different states of the world with the data, according to the model. 

First, let's observe a table of marginals  
```{r}
precis(m4.3)
pairs(m4.3)
```

Plotting the posterior distribution against the data can provide an informal check on assumptions. But more importantly it provides a way to interpret the posterior, especially as models get complicated with interactions and non-linear terms. 

```{r}
plot(height ~ weight, data = d2, col = rangi2)
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map*(x - xbar), add = T)

```

The line we just draw is the posterior mean line and does not capture the uncertainty of the estimates. We can do so by plotting multiple regression lines.  
```{r}
post <- extract.samples(m4.3, n=100)
plot(d2$weight, d2$height, xlim = range(d2$weight), ylim = range(d2$height), col = rangi2, 
     xlab = "Weight", ylab = "Height")
for (i in 1:nrow(post)){
  curve(post$a[i] + post$b[i]*(x - mean(d2$weight)), col = col.alpha("black", 0.3), add = T)
}

```

The uncertainty decreases as we increase the sample size. 
Since $\mu$ is a combination of random variables with distributions, $\mu$ has a distribution. For every value of $x$ weight, we can get all possible values of $\mu$ as a joint distribution of values of $\alpha$ and $\beta$. This distribution incorporates both the individual variabilities of $\alpha$ and $\beta$ as well as any correlation between the variables (joint Gaussian distribution). We can construct an interval for values of height at each values of weight according to our posterior distribution. If we do so for every point of weight in the sample, we can conduct a confidence band for our regression model.  

```{r}
weight.seq <- seq(25, 70, 1)
mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu) # This samples from the posterior of height for each value in weight.seq from the model 

plot(height ~ weight, d2, type = "n")
for(i in 1:100){
  points(weight.seq, mu[i,], pch = 16, col = col.alpha(rangi2,0.1))
}
```

We can summarize the posterior distribution as interval estimates and the mean  

```{r}
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
plot(height~weight, data = d2, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
```

These intervals are great but they're very tight around the posterior mean (MAP). This is because the inferences are always conditional on the model. Even bad models can have tight compatibility intervals. In other words, this line can be thought of as "conditional on the assumption that height and weight are linearly related by a straight line, then this is the most plausible line"  

Some ways we can generate predictions and posterior from a fitted model:  
(1) Use a function (in this case `link`) to generate distributions of posterior values for your parameter $\mu$ for each combination of values from your predictors of interest (in this case, weight).  
(2) Use summary functions like `mean` and `PI` to find averages and lower and upper bounds for $\mu$ for each value of the predictor variable   
(3) Use plotting functionality to draw lines and intervals with respect to the real data.  

### Prediction intervals  
The goal of the model in the prediction sense is not to estimate the average height but more specific heights. The way to do it is: for any unique weight value, you can sample from a Gaussian distribution with the correct mean for that weight, using the value of sigma sampled from the sample posterior distribution. If you do this for every sample from the posterior and every weight value, you can get estimates that incorporates both the uncertainty of the Gaussian distribution, as well as the uncertainty of the posterior.  

```{r}
sim.height <- sim(m4.3, data = list(weight = weight.seq))
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.HPDI, weight.seq)
shade(height.PI, weight.seq)
```  

## Curve from lines  
Here we consider two methods to build curves into our regression. The first is polynomial regression and the second is b-splines.  
### Polynomial regression   
Polynomial regression uses powers of a variable (such as cubes and squares) as extra predictors to build in curvature to the association function.  
Before we start we should start standardizing our variables, which is a common approach. This will ensure that we don't have numerical glitches due to large (or small) numbers, as squares and cubes of variables can be very huge. This should be **default** behavior. Let's define a parabola model on the height vs weight example that we see:   
\[
h_i \sim Normal(\mu_i, \sigma) \\

\mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 \\
\alpha \sim Normal(178,20) \\
\beta_1 \sim LogNormal(0,1) \\
\beta_2 \sim Normal(0,1) \\
\sigma \sim Uniform(0,50) \\
\]
Let's build a model using this new formulation  
```{r}
d$weight_s <- (d$weight - mean(d$weight))/sd(d$weight)

d$weight_s2 <- d$weight_s^2

m4.5 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*weight_s + b2*weight_s2,
    a ~ dnorm(178,20),
    b1 ~ dlnorm(0,1),
    b2 ~ dnorm(0,1),
    sigma ~ dunif(0,50)
  ), data = d
)
precis(m4.5)
```
It's difficult to interpret these polynomial models. $\beta_1$ and $\beta_2$ are square and linear components of the regression, and $\alpha$ is still the intercept but we still don't know what it means. Since this is a polynomial regression, it also doesn't guarantee that we're regressing to the mean of heights either. Let's plot these things out.  

```{r}
weight_seq <- seq(from = -2.2, 2, length.out = 30)
pred_dat <- list(weight_s = weight_seq, weight_s2 = weight_seq^2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- apply(mu,2,mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
height_sim <- sim(m4.5, data = pred_dat)
height.PI <- apply(height_sim, 2, PI, prob = 0.89)

plot(height ~ weight_s, d, col = col.alpha(rangi2, 0.5))
lines(weight_seq,   mu.mean)
shade(mu.HPDI, weight_seq)
shade(height.PI, weight_seq)
```
We can add a third degree polynomial term to get a cubic model. However, these models don't really have explanations as to what they mean, and can actually lead to overfitting.  
**Rethinking: Linear, additive, funky** These models are still linear models even though they are cubic and square terms with non-straight curve. The word "linear" mean different things in different contexts. In this context, the word "linear" here means that $\mu_i$ is a linear function of any single parameter. In other words, $\mu_i$ is linear with respect to $x$ and $x^2$ individually, but the combination of terms makes it non-linear.  

### Splines  
The second way to introduce a curve is through a spline. The word *spline* is a smooth function built out of smaller component piece-wise linear functions. The **B-Spline** we'll look at here is commonplace. **B** here stands for "basis", which means "component".   
```{r}
data(cherry_blossoms)
d <- cherry_blossoms
plot(doy ~ year, data = d)
```
There are many ways you can construct basis functions. Some general ideas:  
(1) First, you define "knots" in your data where basis functions completely transform from one basis to another.  
(2) Then, you can define how many basis you can choose, depends usually on the number of knots.  
(3) The basis functions are supposed to smooth out transitions between knotted sections, and generally no more than two basis functions have an "influence" (non-zero weight) for any point in the predictor variable. This gives splines a "local" effect where locally only one or two parameter has an effect, as supposed to polynomial regression where the slope has an impact across the entire regression.  
(4) You can increase the complexity and flexibility of the model by adding more knots or even use higher degree polynomials. 

With our spline model for cherry blossom data, first we can define some knots. 
```{r knots_definition}
d2 <- d[complete.cases(d$doy), ]
num_knots <- 15
knot_list <- quantile(d2$year, probs = seq(0,1, length.out = num_knots))
knot_list
```
Next we can construct the polynomial degree, which is the number of parameters interacting at a given time. Degree of 1 means that only 2 parameters can interact at a given time (e.g. basis 1 and 2), degree of 2 means 3 parameters can i nteract at a given time, etc. Let's choose the cubic model in this instance.  

```{r basis_splines} 

library(splines)
basis <- bs(d2$year, knots = knot_list[-c(1, num_knots)], degree = 3, intercept = T) # remove the edges 
plot(NULL, xlim=range(d2$year), ylim = c(0,1), xlab = "year", ylab = "basis")
for (i in seq(ncol(basis))){
  lines(d2$year, basis[,i])
}
```

Let's define our model using mathematical form. The model is still the linear regression, and the `bs` function already did the hard work by estimating all the synthetic basis variables.  
\[
  D_i \sim Normal(\mu_i, \sigma) \\
  \mu_i = \alpha + \sum_i^k w_kB_{k,i}\\
  \alpha \sim Normal(100,10) \\
  w_k \sim Normal(0,10) \\
  \sigma \sim Exponential(1) \\
\]
 The priors on $w$ influence how smooth (squiggly) the splines are. We are also using an exponential distribution as a prior. This distribution is good for things like scale parameters that have to be positive. 

```{r fit_spline}
m4.7 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + B %*% w,
    w ~ dnorm(0,10),
    a ~ dnorm(100,10),
    sigma ~ dexp(1)
  ), data = list(D = d2$doy, B = basis), start = list(w = rep(0, ncol(basis)))
)
```

Let's plot the predictive posterior distribution  

```{r spline_plots}
post <- extract.samples(m4.7)
w <- apply(post$w, 2, mean) 

plot(NULL, xlim = range(d2$year), ylim = c(-6,6), xlab = "Year", ylab = "Basis * weight", main = "Relative weights")
for (i in 1:ncol(basis)){
  lines(d2$year, w[i]*basis[,i])
}

mu <- link(m4.7)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
plot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16, main = "Mean year estimate")
shade(mu.PI, d2$year, col = col.alpha("black", 0.3))

```

## Practice Problems  

**4E1**: The likelihood is line 1 $y_i \sim Normal(\mu,\sigma)$  
**4E2**: The posterior distribution has two parameters  
**4E3**: Bayes theorem in this form can be:  
$$P(\mu,\sigma|y) = \frac{\prod dnorm(\mu, \sigma)*dnorm(0,10)*dexp(1)}{\int\prod dnorm(\mu, \sigma)*dnorm(0,10)*dexp(1) d\mu d\sigma}$$
**4E4** The linear model is line 2 $\mu_i = \alpha + \beta x_i$  
**4E5** In the model above, the posterior distribution has 3 variables  
**4M1**  Simulate observed y values from the prior (not the posterior)  
Model: 
\[
  y_i \sim Normal(\mu, \sigma) \\
  \mu \sim Normal(0,10) \\
  \sigma\sim Exponential(1)
\]

```{r 4m1}
ysim <- rnorm(1000, mean = rnorm(1000,0,10), sd = rexp(1000))
dens(ysim)
```

**4M2** Translate model into a quap formula  
```{r 4m2}
m.4m2 <- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0,10),
  sigma ~ dexp(1)
)
```  

**4M3** Translate quap into math definitions  
\[
  y_i \sim Normal(\mu_i, \sigma) \\
  \mu_i = \alpha + \beta x_i \\
  \alpha \sim Normal(0,10) \\
  \beta \sim Uniform(0,1) \\
  \sigma \sim Exponential(1)
\]

**4M4** Regression using height and year  
Model can be conceptualized as  
\[
  height \sim Normal(\mu_i, \sigma); \\
  \mu = \alpha + \beta(x_i - \bar{x}); \\
  \alpha \sim Normal(178,10); \\
  \beta \sim LogNormal(0,1); \\
  \sigma \sim Exponential(1);
  
\]
We choose the prior for alpha as the height if the year is the average year. Then, it should approximately be the general average height, which we can say is normally distributed (height is normally distributed) with parameters 
178 and 20 similar to the problem before. For beta, we choose lognormal because beta can't be negative (height can't decrease with years), and 0,1 bound gives the best estimate.  

**4M5** Yes. We will change our prior for lognormal such that the rate of increase in height wrt year cannot be 0.  
**4M6** Yes. We can revise our prior for our sigma as never deviating from 64cm which means it should be $Exponential(1/64)$.  
**4M7** Refit model m4.3  

```{r 4m7}
d2 <- Howell1 %>% filter(age >= 18)
m.4m7 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(178,20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data = d2
)
m4.3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar),
    a ~ dnorm(178,20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data = d2
)

precis(m.4m7)
precis(m4.3)
weight_seq <- seq(25, 70, 1)
sim_4m7 <- sim(m.4m7, data = list(weight = weight_seq))
sim_43 <- sim(m4.3, data = list(weight = weight_seq))

PI_4m7 <- apply(sim_4m7, 2, PI, prob = 0.89)
PI_43 <- apply(sim_43, 2, PI, prob = 0.89)

plot(height ~ weight, data = d2, pch = 16)
shade(PI_4m7, weight_seq, col = col.alpha("blue", 0.2))
shade(PI_43, weight_seq, col = col.alpha("red", 0.2))


```
The posterior predictive distributions are very similar. The only difference is in the intercept. In the original model, the intercept is the value of height is when weight is equal to the mean value of weight. The second model is essentially what is the value of height when weight is 0. This parameter obviously doesn't make sense (which is why a 0 intercept model usually standardizes their predictors).  

**4M8**  
```{r 4m8}
library(splines)
library(RColorBrewer)
data(cherry_blossoms)
d <- cherry_blossoms
d2 <- d[complete.cases(d$doy), ]

colors <- brewer.pal(4, "Spectral")

get_model <- function(data, num_knots, w_var){
  knot_list <- quantile(data$year, probs = seq(0,1, length.out = num_knots))
  basis <- bs(data$year, knots = knot_list[-c(1, num_knots)], degree = 3, intercept = T) # remove the edges
  model <- quap(
    alist(
      D ~ dnorm(mu, sigma),
      mu <- a + B %*% w,
      w ~ dnorm(0,w_var),
      a ~ dnorm(100,10),
      sigma ~ dexp(1)
    ), data = list(D = data$doy, B = basis, w_var = w_var), start = list(w = rep(0, ncol(basis)))
  )
  mu <- link(model)
  mu_PI <- apply(mu, 2, PI, prob = 0.89)
  return(mu_PI)
}

print("First, let's vary the number of knots")
knots <- c(10,15, 25,30)

mod_list <- list()
for (i in seq(length(knots))){
 mod_list[[i]] <- get_model(d2, knots[i], 10)
}

plot(doy ~ year, data = d2, main = "Different means")
for (i  in seq(length(knots))){
  shade(mod_list[[i]], d2$year, col = col.alpha(colors[i], alpha = 0.4))
}


variance <- c(5,10,20,30)
for (i in 1:length(variance)){
  mod_list[[i]] <- get_model(d2, 15, variance[i])
}

plot(doy~year, data = d2, main = "Different variances")
for (i in seq(length(variance))){
  shade(mod_list[[i]], d2$year, col = col.alpha(colors[i], alpha = 0.4))
}


```
Different variance parameters didn't really change the estimations, however more knots increases how smooth the function can be.  

**4H1** Fill the table using model based predictions  
```{r 4h1}
data("Howell1")
d2 <- Howell1 %>% filter(age >= 18)
xbar <- mean(d2$weight)
model <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar),
    a ~ dnorm(178,20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data = d2
)

weights_list <- c(46.95, 43.72, 64.78, 32.59, 54.63)
sim <- sim(model, data = list(weight = weights_list))
sim_mean <- apply(sim,2, mean)
sim_PI <- apply(sim, 2, PI, prob = 0.89)


cbind(weights_list, sim_mean, t(sim_PI))



```
**4H2** 
```{r 4h2}
d3 <- Howell1 %>% filter(age < 18)
xbar <- mean(d3$weight)
model <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar),
    a ~ dnorm(150,20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data = d3
)
precis(model)
```
We can say that every 10 units increase in weight would correspond to 27.2 increase in units in height.  
```{r 4h2_2}
sim <- sim(model)
mu <- link(model)
mu_mean <- apply(mu, 2, mean)
mu_PI <- apply(mu, 2, PI, prob = 0.89) %>% t() %>% as.data.frame() %>% rename("mu_lower" = "5%", "mu_upper" = "94%")
sim_PI <- apply(sim, 2, PI, prob = 0.89) %>% t() %>% as.data.frame() %>% rename("sim_lower" = "5%", "sim_upper" = "94%")

cbind(mu_mean, mu_PI, sim_PI, d3) %>% ggplot(aes(x = weight, y = height)) + geom_point() + geom_line(aes(x = weight, y = mu_mean)) + geom_ribbon(aes(ymax = mu_upper, ymin = mu_lower), fill = "steelblue", alpha = 0.3) + 
  geom_ribbon(aes(ymax = sim_upper, ymin = sim_lower), fill = "salmon", alpha = 0.3) + theme_bw()
```

The model seems to be trying to estimate a non-linear relationship using a linear model. The higher values are extrapolated linearly however there is a plateau effect at higher weights. A better model would have been to use a different functional form such as polynomial or spline regression.    
**4H3**  
```{r 4h3_1}
d4 <- Howell1 
d4$weight <- log(d4$weight)
xbar <- mean(d4$weight)
model <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight - xbar),
    a ~ dnorm(150,20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ), data = d4
)
precis(model)
```

For every increase in log-kg, we get an increase of 47.07 cm in height  
b. Plotting  
```{r 4h3_2}
mu_interval <- apply(link(model, data = list(weight = d4$weight)),2, PI, prob = 0.97)
sim_height <- sim(model, data = list(weight = d4$weight))
means <- apply(sim_height, 2, mean)
interval <- apply(sim_height, 2, PI, prob = 0.97)
d4 <- d4 %>% mutate(sim_mu = means, upper = interval[1,], lower = interval[2,], true_weight = Howell1$weight, 
                    lower_mu = mu_interval[1,], upper_mu = mu_interval[2,])
ggplot(d4, aes(x = true_weight, y = height)) + geom_point(col = "salmon") + 
  geom_point(aes(x = true_weight, y = sim_mu), col = "steelblue") + theme_bw() + 
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3, fill = "steelblue") + 
  geom_ribbon(aes(ymin = lower_mu, ymax = upper_mu), alpha = 0.3, fill = "salmon") + 
  labs(x = "Weight", y = "Height")
```
**4H4**   
Getting the prior predictive simulation for parabola model   
```{r 4h4_1}
d5 <- Howell1
N <- 100
b1 <- rlnorm(N,0,1)
b2 <- rnorm(N,0,1)
alpha <- rnorm(N, 178,200)

df <- tibble(x = d5$weight)

for (i in 1:N){
  y_vals <- alpha[i] + b1[i] * d5$weight + b2[i]*(d5$weight^2)
  name <- paste0("rep",i)
  df <- df %>% mutate(!!name := y_vals)
}
df <- df %>% pivot_longer(starts_with("rep"))

ggplot(df, aes(x = x, y = value, group = name)) + geom_line(alpha = 0.5) + labs(x = "Weight", y = "Height") + theme_bw()

```
Can modify the prior of beta 2 to be positive only  
```{r 4h4_2`}
N <- 100
b1 <- rlnorm(N,0,1)
b2 <- rlnorm(N,0,1)
alpha <- rnorm(N, 178,200)

df <- tibble(x = d5$weight)

for (i in 1:N){
  y_vals <- alpha[i] + b1[i] * d5$weight + b2[i]*(d5$weight^2)
  name <- paste0("rep",i)
  df <- df %>% mutate(!!name := y_vals)
}
df <- df %>% pivot_longer(starts_with("rep"))

ggplot(df, aes(x = x, y = value, group = name)) + geom_line(alpha = 0.5) + labs(x = "Weight", y = "Height") + theme_bw()

```
**4H5**  
```{r 4h5_1}
data(cherry_blossoms)
d6 <- cherry_blossoms
qplot(y = doy, x = temp, data = d6)
```
```{r 4h5_2}


```

